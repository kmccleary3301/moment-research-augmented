#Experiment parameters
random_seed: 13
notes: "Pre-training run with reduced dataset"

# Encoder (MOMENT) parameters
task_name: "pre-training" 

# Model parameters


# MOMENT parameters
model_name: "MOMENT"
seq_len: 512
patch_len: 8
patch_stride_len: 8
revin_affine: False
d_model: 512
e_layers: 6
dropout: 0.1
attention_dropout: 0.1
output_attention: False
n_heads: 8
d_ff: 2048
activation: "relu"


torch_dtype: "bfloat16"
value_embedding_bias: False # Whether to add biases to the value embeddings
orth_gain: 1.41
add_positional_embedding: False # Wheter to add positional embedding to the input
set_input_mask: True # Whether to set the input mask
randomly_initialize_backbone: True # Whether to randomly initialize the encoder
transformer_type: "encoder_only" # "encoder_decoder" "encoder_only" "decoder_only"
transformer_backbone: "SparseAttentionEncoder"
original_transformer_backbone: "google/flan-t5-small"

freeze_transformer_backbone: False # Whether to freeze the transformer backbone


# Logging
log_interval: 500  # Reduced from 100000
checkpoint_interval: 500  # Reduced from 5000
debug: True  # Changed from False to True to get more debug information

# Training parameters
mask_ratio: 0.3
optimizer_name: AdamW
enable_gradient_checkpointing: True
max_norm: 1.0
weight_decay: 0.05
lr_scheduler_type: linearwarmupcosinelr
init_lr: 0.00005
min_lr: 0.000005
lr_decay_rate: 0.9
warmup_lr: 0.000005
warmup_steps: 200
use_amp: True
max_epoch: 2  # Reduced from 50

# Dataset parameters
target_col : 'OT'
scale : True
data_stride_len : 1 
train_ratio : 0.6
val_ratio : 0.1
test_ratio : 0.3
upsampling_pad_direction : "backward"
upsampling_type : "pad"
downsampling_type : "interpolate"
pad_mode : "constant"
pad_constant_values : 0
dataset_names : 'all'
datasets_fraction : 1.00
train_batch_size : 512  # Reduced from 2048
val_batch_size: 512  # Reduced from 3072
shuffle : True
num_workers : 5
pin_memory : True
drop_last : False
seed: 42

use_wandb: True